---
title: "FederalistPapers-part2"
author: "Ryan Wesslen"
date: "August 7, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## Step 6: Rerun preparation steps.

Let's rerun our initial steps. If you are manually running the chunks, you may not need to rerun this process.

```{r}
setwd("~/Dropbox/Federalist-Papers-Workshop")  #Linux
#setwd("C:/Users/rwesslen/Dropbox/Federalist-Papers-Workshop") #Windows

papers <- read.csv("./data/federalist.csv", stringsAsFactors = F)

#		train/test: limit to Hamilton/Mad documents (drop Jay and Ham&Madison papers)
known.papers <- which(papers$auth=="HAMILTON" | papers$auth=="MADISON")
length(known.papers)	# 65

disputed <- which(papers$auth=="DISPUTED")
length(disputed)	# 12

library(quanteda)

# remove "To the People of the State of New York <l>"
papers$text <- substring(papers$text,45)

# remove html tags
tags <- c("<c>","<p>","<S>","<l>","<q>","<d>","<Q>")
for (i in tags){
  papers$text <- gsub(i," ",papers$text)
}

# build the corpus
myCorpus <- corpus(papers$text)

# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author

# summarize the documents
summary(myCorpus, 5)

stopWords <- c("will","one","two","may","less", "well","might","without","small","single", "several","however","must","number","part","upon","consider","particular","place","true","shall","often","former","latter","want","can","everything","every","different","either","yet","made","now","publius","therefore","first","second","third","though","another","found","within","even","far","just","also","said","ever","three","four","still","little","federal","members","national","union","united","general","government","governments","power","powers","state","states","people","constitution","constitutions")

myDfm <- dfm(myCorpus, ignoredFeatures = c(stopwords("english"),stopWords), stem = T, ngrams = c(1,3))

myDfm <- trim(myDfm, minCount=10, minDoc=5)
```

## Step 7: Data Partitioning & Train Ridge Regression

In machine learning, the most common problem is overfitting. Overfitting is when the model accidentally fits random noise into the model results. The problem then comes when applying the model to a new or out-of-sample dataset. If a model is overfit, it will likely perform well on the original model, or training, dataset but will perform poorly on the out-of-sample dataset.

```{r}
set.seed(123)
training <- sample(1:length(known.papers), floor(.80 * length(known.papers)))
test <- (1:length(known.papers))[1:length(known.papers) %in% training == FALSE]
```

This part below we'll use ridge regression to determine what are the most "predictive" words for either Madison or Hamilton. This part uses material originally from [Pablo Barbera's EUI Text Workshop](https://github.com/pablobarbera/eui-text-workshop/blob/master/02-supervised/01-supervised.Rmd).

```{r}
training.author <- ifelse(papers$author[training]=="HAMILTON",1,0)

library(glmnet)
require(doMC)
registerDoMC(cores=3)
ridge <- cv.glmnet(myDfm[training,], training.author, 
    family="binomial", alpha=0, nfolds=5, parallel=TRUE,
    type.measure="deviance")
plot(ridge)
```

This plots a range of lambda values to determine the most appropriate value to use.

## Step 8: Model Evaluation

Let's now predict the accuracy of the model on the training dataset.

```{r}
## function to compute accuracy
accuracy <- function(ypred, y){
    tab <- table(ypred, y)
    return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
    tab <- table(ypred, y)
    return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
    tab <- table(ypred, y)
    return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
```

Let's predict the training dataset.

```{r}
# computing predicted values
preds <- predict(ridge, myDfm[training,], type="response") > mean(training.author)
# confusion matrix
table(preds, training.author)
# performance metrics
accuracy(preds, training.author)
precision(preds, training.author)
recall(preds, training.author)
```

So far so good. Out of the 52 training papers, the model was perfect.

But let's now see how the model does for the out-of-sample dataset.

```{r}
test.author <- ifelse(papers$author[test]=="HAMILTON",1,0)

# computing predicted values
preds <- predict(ridge, myDfm[test,], type="response") > mean(test.author)
# confusion matrix
table(preds, test.author)
# performance metrics
accuracy(preds, test.author)
precision(preds, test.author)
recall(preds, test.author)
```

Good but not perfect. There were 13 out of sample observations and the model correctly 12 out of 13 (92.3%).   

So we find that the model completely predicts the training dataset. Usually, we'd be more interested in the validation dataset accuracy; however, since we have so few training observations, we only consider the training dataset.

We can now predict our out-of-sample (disputed) papers.

First, predict the likelihood that the paper was written by Hamilton:

```{r}
predict(ridge, myDfm[disputed,], type="response")
```

For the training dataset, what was the ratio of Hamilton to Madison papers?

```{r}
mean(c(training.author,test.author))
```

Therefore, if we choose all papers that have a predicted probability below the training Hamilton paper we predict that...

```{r}
disputed.papers <- predict(ridge, myDfm[disputed,], type="response") > mean(c(training.author,test.author))

disputed.papers <- ifelse(disputed.papers == 1, "HAMILTON", "MADISON")
```

Similar to earlier research that attributed all papers to Madison (Mosteller & Wallace 1963), our model predicts Madison wrote all twelve of the disputed papers.

The model's covariates (X or independent variables) are stemmed words. 

We'll choose the best lambda value (regression run) and explore five of the beta's.

```{r}
# from the different values of lambda, let's pick the best one
best.lambda <- which(ridge$lambda==ridge$lambda.min)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
```

Let's now explore the top 30 words per author (i.e. most negative and most positive beta coefficients).

```{r}
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
                word = names(beta), stringsAsFactors=F)

df <- df[order(df$coef),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
df <- df[order(df$coef, decreasing=TRUE),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
```

These plots show the top 30 words that the model predicts indicates Madison (first 30, i.e. most negative coefficient) or Hamilton (bottom 30, i.e. most positive coefficient).

## Advanced Questions:

How robust are the results? 

What happens if you alter the pre-processing steps, like: 

* add or remove more stop words

* remove stemming

* remove more or less sparse words (currently have to be 10 times and in 5 docs)

Would a different model (e.g. Naive Bayes) perform similarily?