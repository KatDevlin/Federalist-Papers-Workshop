install.packages("quanteda")
install.packages("tm")
install.packages("dplyr")
install.packages("plyr")
library(quanteda)
?similarity
shiny::runApp('Dropbox/UNCC_SocialScience_Collaboration/socsci_shiny')
shiny::runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
shiny::runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/ShinyDemo/LDAModelApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
shiny::runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
tdfm$Company
MyCorpus$metadata
myCorpus$metadata["Company"]
MyCorpus$metadata["Company"]
MyCorpus$metadata['Company']
MyCorpus$metadata
summary(myCorpus, n = 5, showmeta = TRUE)
summary(MyCorpus, n = 5, showmeta = TRUE)
Q
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
shiny::runApp('Dropbox/ProjectMosaicTool')
colleges[input$topic.college]
colleges
input$topic.college
colleges[as.numeric(input$topic.college)]
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
Dfm <- dfm(Corpus, groups = "Author", ignoredFeatures = c(stopwords("english"), stp),
stem = input$stemming, removeTwitter = input$twitter.stop, ngrams=c(1,input$unigrams))
text$StandardAuthor
text$Department
text$College
College %in% colleges[input$topic.college]
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
xx <- subset(Corpus, College %in% colleges[input$topic.college])
colleges[input$topic.college]
input$topic.college
runApp('Dropbox/ProjectMosaicTool')
colleges[as.numeric(input$topic.college)]
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
xx <- topic.network(v$probterms, v$probdocs, input$parm, v$topicnames)
runApp('Dropbox/ProjectMosaicTool')
unlist(strsplit(trimws(tweet.stopwords),","))
runApp('Dropbox/ProjectMosaicTool')
xx <- topic.network(v$probterms, v$probdocs, input$parm, v$topicnames)
Q
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
tdata2 <- data.frame(ID = tdata[,5], Topic_Prob = round(temp,3), stringsAsFactors = F)
order(tdata2$Topic_Prob,decreasing = T)
v$probdocs[,c(input$topic.network_selected)]
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
summary(z$Corpus, length(temp))
?dfm
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
runApp('Dropbox/ProjectMosaicTool')
shiny::runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
trimws(input$stopwords)
runApp('Dropbox/MNEs/Analyses/Ryan/Domestic/DomesticShinyApp')
library(shiny)
?bootstrapPage
print(source('Desktop/MyShinyApp.R')$value)
print(source('Desktop/MyShinyApp.R')$value)
print(source('Desktop/MyShinyApp.R')$value)
print(source('Desktop/MyShinyApp.R')$value)
print(source('Desktop/MyShinyApp.R')$value)
runExample("01_hello")
runExample("08_html")
runExample("04_mpg")
runExample("11_timer")
runExample("06_tabsets")
dotR <- file.path(Sys.getenv("HOME"), ".R")
if (!file.exists(dotR)) dir.create(dotR)
M <- file.path(dotR, "Makevars")
if (!file.exists(M)) file.create(M)
cat("\nCXXFLAGS=-O3 -mtune=native -march=native -Wno-unused-variable -Wno-unused-function",
file = M, sep = "\n", append = TRUE)
install.packages("rstan", repos = "https://cloud.r-project.org/", dependencies=TRUE)
devtools::install_github("stan-dev/shinystan", build_vignettes = TRUE)
library("shinystan")
launch_shinystan(my_fit)
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
# Chunk 2
setwd("~/Dropbox/Federalist-Papers-Workshop")  #Linux
#setwd("C:/Users/rwesslen/Dropbox/Federalist-Papers-Workshop") #Windows
papers <- read.csv("./data/federalist.csv", stringsAsFactors = F)
#		train/test: limit to Hamilton/Mad documents (drop Jay and Ham&Madison papers)
train <- which(papers$auth=="HAMILTON" | papers$auth=="MADISON")
length(train)	# 65
test <- which(papers$auth=="DISPUTED")
length(test)	# 12
library(quanteda)
# remove "To the People of the State of New York <l>"
papers$text <- substring(papers$text,45)
# remove html tags
tags <- c("<c>","<p>","<S>","<l>","<q>","<d>","<Q>")
for (i in tags){
papers$text <- gsub(i," ",papers$text)
}
# build the corpus
myCorpus <- corpus(papers$text)
# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author
# summarize the documents
summary(myCorpus, 5)
stopWords <- c("will","one","two","may","less", "well","might","without","small","single", "several","however","must","number","part","upon","consider","particular","place","true","shall","often","former","latter","want","can","everything","every","different","either","yet","made","now","publius","therefore","first","second","third","though","another","found","within","even","far","just","also","said","ever","three","four","still","little","federal","members","national","union","united","general","government","governments","power","powers","state","states","people","constitution","constitutions")
myDfm <- dfm(myCorpus, ignoredFeatures = c(stopwords("english"),stopWords), stem = F, ngrams = c(1,3))
myDfm <- trim(myDfm, minCount=10, minDoc=5)
# Chunk 3
# we now export to a format that we can run the topic model with
dtm <- convert(myDfm, to="topicmodels")
# Chunk 4
# install.packages("topicmodels")
library(topicmodels)
#Set parameters for Gibbs sampling
burnin <- 1000
iter <- 4000
thin <- 100
seed <- 40
#Number of Topics
k <- 20
#Run LDA using Gibbs sampling
lda <-LDA(dtm,k, method='Gibbs',
control=list(seed = seed, burnin = burnin, iter = iter, thin = thin))
# Chunk 5
topicmodels_json_ldavis <- function(fitted, dfm, dtm){
# Required packages
library(topicmodels)
library(dplyr)
library(stringi)
library(quanteda)
library(LDAvis)
library(tm)
# Find required quantities
phi <- posterior(fitted)$terms %>% as.matrix
theta <- posterior(fitted)$topics %>% as.matrix
vocab <- colnames(phi)
doc_length <- ntoken(dfm[rownames(dtm)])
temp_frequency <- inspect(dtm)
freq_matrix <- data.frame(ST = colnames(temp_frequency),
Freq = colSums(temp_frequency))
rm(temp_frequency)
# Convert to json
json_lda <- LDAvis::createJSON(phi = phi, theta = theta,
vocab = vocab,
doc.length = doc_length,
term.frequency = freq_matrix$Freq)
return(json_lda)
}
# Chunk 6
#Create Json for LDAVis
json <- topicmodels_json_ldavis(lda, myCorpus, dtm)
new.order <- RJSONIO::fromJSON(json)$topic.order
# Chunk 7
term <- terms(lda, 10)
term <- term[,new.order]
colnames(term) <- paste("Topic",1:20)
term
term <- terms(lda, 10)
term <- term[,new.order]
colnames(term) <- paste("Topic",1:20)
term
View(term)
term <- terms(lda)
term <- term[,new.order]
colnames(term) <- paste("Topic",1:20)
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics[,new.order])
View(probtopics)
probterms <- data.frame(postlist$terms[,new.order])
probterms <- data.frame(postlist$terms[new.order,])
probterms <- data.frame(t(postlist$terms[new.order,]))
View(probterms)
colnames(probterms) <- paste("Topic",1:20)
cor_threshold <- .2
cor_mat <- cor(probterms)
corrPlots <- function(probterms){
cor_threshold <- .2
cor_mat <- cor(probterms)
cor_mat[ cor_mat < cor_threshold ] <- 0
diag(cor_mat) <- 0
library(igraph)
graph <- graph.adjacency(cor_mat, weighted=TRUE, mode="lower")
E(graph)$edge.width <- E(graph)$weight
V(graph)$label <- paste(1:20)
par(mar=c(0, 0, 3, 0))
set.seed(110)
plot.igraph(graph, edge.width = E(graph)$edge.width,
edge.color = "blue", vertex.color = "white", vertex.size = 1,
vertex.frame.color = NA, vertex.label.color = "grey30")
title("Strength Between Topics Based On Word Probabilities", cex.main=.8)
}
corrPlots(probterms)
cor_threshold <- .2
cor_mat <- cor(probterms)
cor_mat[ cor_mat < cor_threshold ] <- 0
diag(cor_mat) <- 0
library(igraph)
graph <- graph.adjacency(cor_mat, weighted=TRUE, mode="lower")
E(graph)$edge.width <- E(graph)$weight
V(graph)$label <- paste(1:20)
par(mar=c(0, 0, 3, 0))
set.seed(110)
plot.igraph(graph, edge.width = E(graph)$edge.width,
edge.color = "blue", vertex.color = "white", vertex.size = 1,
vertex.frame.color = NA, vertex.label.color = "grey30")
title("Strength Between Topics Based On Word Probabilities", cex.main=.8)
E(graph)$weight
View(cor_mat)
cor_mat <- cor(probterms)
View(cor_mat)
cor_threshold <- .1
cor_mat <- cor(probterms)
cor_mat[ cor_mat < cor_threshold ] <- 0
diag(cor_mat) <- 0
library(igraph)
graph <- graph.adjacency(cor_mat, weighted=TRUE, mode="lower")
E(graph)$edge.width <- E(graph)$weight
V(graph)$label <- paste(1:20)
par(mar=c(0, 0, 3, 0))
set.seed(110)
plot.igraph(graph, edge.width = E(graph)$edge.width,
edge.color = "blue", vertex.color = "white", vertex.size = 1,
vertex.frame.color = NA, vertex.label.color = "grey30")
title("Strength Between Topics Based On Word Probabilities", cex.main=.8)
cor_threshold <- .08
cor_mat <- cor(probterms)
cor_mat[ cor_mat < cor_threshold ] <- 0
diag(cor_mat) <- 0
library(igraph)
graph <- graph.adjacency(cor_mat, weighted=TRUE, mode="lower")
E(graph)$edge.width <- E(graph)$weight
V(graph)$label <- paste(1:20)
par(mar=c(0, 0, 3, 0))
set.seed(110)
plot.igraph(graph, edge.width = E(graph)$edge.width,
edge.color = "blue", vertex.color = "white", vertex.size = 1,
vertex.frame.color = NA, vertex.label.color = "grey30")
title("Strength Between Topics Based On Word Probabilities", cex.main=.8)
library(d3heatmap)
# Cluster topics (rows)
d3heatmap(data.frame(heat_data), dendrogram = "col", scale = "col",
labRow = name, cexRow=.8, cexCol=.8, k_row = 5)
# Cluster topics (rows) and papers (columns)
d3heatmap(data.frame(heat_data), scale = "col",
labRow = name, cexRow=.8, cexCol=.8, k_row = 5)
# Name (label) the topics
names <- c("Legalese","War/National Security","Enlightenment","House Term Period","Military & Army","Judicial Branch","Public Opinion","Republican Majority","Clauses","Known Issues","Senate","Conduct of Man","Revenue","Local Rights","Convention","Checks & Balances","Foreign Assets","History","Property Rights","States Rights")
## Topic Cluster heatmaps
# to get topic probabilities per document
postlist <- posterior(lda)
probtopics <- data.frame(postlist$topics[,new.order])
heat_data <- as.matrix(probtopics)
colnames(heat_data) <- names
levels(papers$author)[2] <- "BOTH"
levels(papers$author)[3] <- "???"
name <- paste(papers$number,". ",papers$author)
library(d3heatmap)
# Cluster topics (rows)
d3heatmap(data.frame(heat_data), dendrogram = "col", scale = "col",
labRow = name, cexRow=.8, cexCol=.8, k_row = 5)
# Cluster topics (rows) and papers (columns)
d3heatmap(data.frame(heat_data), scale = "col",
labRow = name, cexRow=.8, cexCol=.8, k_row = 5)
View(probtopics)
?serVis
