install.packages("quanteda")
install.packages("tm")
install.packages("dplyr")
install.packages("plyr")
library(quanteda)
?similarity
setwd("~/Dropbox/Federalist-Papers-Workshop")  #Linux
install.packages("quanteda")
papers <- read.csv("./data/federalist.csv", stringsAsFactors = F)
dim(papers); names(papers)
table(papers$author)
text <- substring(papers$text,45)
text <- substring(papers$text,45)
# build the corpus
myCorpus <- corpus(text)
# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author
# summarize the documents
summary(myCorpus, 5)
library(quanteda)
# remove "To the People of the State of New York <l>"
text <- substring(papers$text,45)
# build the corpus
myCorpus <- corpus(text)
# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author
# summarize the documents
summary(myCorpus, 5)
tokenInfo <- summary(myCorpus)
View(tokenInfo)
View(papers)
if (require(ggplot2))
ggplot(data=tokenInfo, aes(x=Number, y=Tokens, group=1)) + geom_line() + geom_point())
if (require(ggplot2))
ggplot(data=tokenInfo, aes(x=Number, y=Tokens, group=1)) + geom_line() + geom_point()
?dfm
View(papers)
text[1]
tags <- c("<c>","<p>","<s>","<l>","<q>","<d>")
for (i in tags){
text <- gsub(tags[i]," ", text)
}
text <- substring(papers$text,45)
tags[1]
gsub(tags[i]," ",text)
gsub("<p>","  ",text[1])
gsub(tags[i]," ",papers$text)
gsub(tags[i]," ",papers$text[1])
gsub(tags[i]," ",papers$text)
gsub(tags[i]," ",papers$text)
gsub(tags[i]," ",papers$text)
gsub(tags[i]," ",papers$text)
gsub(tags[i]," ",papers$text)
text <- gsub(tags[i],"  ",papers$text)
gsub(tags[i],"  ",papers$text[1])
tags[i]
text <- gsub(i,"",papers$text)
tags <- c("<c>","<p>","<s>","<l>","<q>","<d>")
for (i in tags){
papers$text <- gsub(i," ",papers$text)
}
View(papers)
papers <- read.csv("./data/federalist.csv", stringsAsFactors = F)
papers$text <- substring(papers$text,45)
tags <- c("<c>","<p>","<S>","<l>","<q>","<d>")
for (i in tags){
papers$text <- gsub(i," ",papers$text)
}
View(papers)
tags <- c("<c>","<p>","<S>","<l>","<q>","<d>","<Q>")
for (i in tags){
papers$text <- gsub(i," ",papers$text)
}
myCorpus <- corpus(papers$text)
# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author
# summarize the documents
summary(myCorpus, 5)
View(papers)
tokenInfo <- summary(myCorpus)
if (require(ggplot2))
ggplot(data=tokenInfo, aes(x=Number, y=Tokens, group=1)) + geom_line() + geom_point()
stopWords <- c("will","one","two","may","less", "well","might","without","small","single",
"several","however","must","number","part","upon","consider","particular","place","true","shall","often",
"former","latter","want","can","everything","every","different","either","yet","made","now","publius",
"therefore","first","second","third","though","another","found","within","even","far","just","also",
"said","ever","three","four","still","little")
myDfm <- dfm(myCorpus, ignoredFeatures = c(stopwords("english"),stopWords), stem = F)
topfeatures(myDfm, 20)
if (require(RColorBrewer))
plot(myDfm, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
if (require(RColorBrewer))
plot(myDfm, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
if (require(RColorBrewer))
plot(tfidf(myDfm), max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
myGroupDfm <- dfm(myCorpus, groups = "Author" ignoredFeatures = c(stopwords("english"),stopWords), stem = F)
myGroupDfm <- dfm(myCorpus, groups = "Author", ignoredFeatures = c(stopwords("english"),stopWords), stem = F)
plot(myGroupDfm, comparison = T, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
plot(tfidf(myGroupDfm), comparison = T, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
plot(myGroupDfm, comparison = T, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
papers$author[papers$author == "HAMILTON AND MADISON"] <- "BOTH"
papers$author[papers$author == "HAMILTON OR MADISON"] <- "DISPUTED"
papers <- read.csv("./data/federalist.csv", stringsAsFactors = F)
papers$author[papers$author == "HAMILTON AND MADISON"] <- "BOTH"
papers$author[papers$author == "HAMILTON OR MADISON"] <- "DISPUTED"
write.csv(papers, file = "./data/federalist.csv", row.names = F)
papers <- read.csv("./data/federalist.csv", stringsAsFactors = F)
dim(papers)
table(papers$author)
#		train/test: limit to Hamilton/Mad documents (drop Jay and Ham&Madison papers)
train <- which(papers$auth=="HAMILTON" | papers$auth=="MADISON")
length(train)	# 65
test <- which(papers$auth=="DISPUTED")
length(test)	# 12
papers$text <- substring(papers$text,45)
# remove html tags
tags <- c("<c>","<p>","<S>","<l>","<q>","<d>","<Q>")
for (i in tags){
papers$text <- gsub(i," ",papers$text)
}
# build the corpus
myCorpus <- corpus(papers$text)
# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author
# summarize the documents
summary(myCorpus, 5)
stopWords <- c("will","one","two","may","less", "well","might","without","small","single",
"several","however","must","number","part","upon","consider","particular","place","true","shall","often",
"former","latter","want","can","everything","every","different","either","yet","made","now","publius",
"therefore","first","second","third","though","another","found","within","even","far","just","also",
"said","ever","three","four","still","little")
myDfm <- dfm(myCorpus, ignoredFeatures = c(stopwords("english"),stopWords), stem = F)
topfeatures(myDfm, 20)
plot(myDfm, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
plot(tfidf(myDfm), max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
myGroupDfm <- dfm(myCorpus, groups = "Author", ignoredFeatures = c(stopwords("english"),stopWords), stem = F)
plot(myGroupDfm, comparison = T, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
plot(tfidf(myGroupDfm), comparison = T, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
plot(myGroupDfm, comparison = T, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
source("https://bioconductor.org/biocLite.R")
biocLite("graph")
biocLite("Rgraphviz")
DTM <- as.DocumentTermMatrix(myDfm)
findAssocs(DTM, "commerce", 0.7)
library(tm)
findAssocs(DTM, "commerce", 0.7)
freq.terms <- findFreqTerms(DTM, lowfreq = 200)
term.freq <- rowSums(as.matrix(DTM))
term.freq <- subset(term.freq, term.freq >= 200)
plot(DTM, term = freq.terms, corThreshold = 0.25, weighting = T)
myDfm <- trim(myDfm, minCount=5, minDoc=3)
# hierarchical clustering - get distances on normalized dfm
myDistMat <- dist(as.matrix(weight(myDfm, "relFreq")))
# hiarchical clustering the distance object
myCluster <- hclust(myDistMat)
# label with document names
myCluster$labels <- docnames(myCluster)
# plot as a dendrogram
plot(myCluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
plot(myCluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
plot(tfidf(myCluster), xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
kfit <- kmeans(myDistMat, 7)
clusplot(as.matrix(myDistMat), kfit$cluster, color=T, shade=T, labels=2, lines=0)
stopWords <- c(stopWords,"federal","members","national","union","united","general","government","governments",
"power","powers","state","states","people","constitution","constitutions")
myDfm <- dfm(myCorpus, ignoredFeatures = c(stopwords("english"),stopWords), stem = T, ngrams = c(1,3))
plot(myDfm, max.words = 100, colors = brewer.pal(6, "Dark2"), scale = c(8, .5))
topfeatures(myDfm, 20)
## Step 5: Additional Stop Words, Stemming and Bigrams
DTM <- as.DocumentTermMatrix(myDfm)
DTM <- as.DocumentTermMatrix(myDfm, weighting = "relFreq")
?as.DocumentTermMatrix
DTM <- DocumentTermMatrix(myDfm)
papers <- read.csv("../data/federalist.csv", stringsAsFactors = F)
#		train/test: limit to Hamilton/Mad documents (drop Jay and Ham&Madison papers)
train <- which(papers$auth=="HAMILTON" | papers$auth=="MADISON")
length(train)	# 65
test <- which(papers$auth=="DISPUTED")
length(test)	# 12
library(quanteda)
# remove "To the People of the State of New York <l>"
papers$text <- substring(papers$text,45)
# remove html tags
tags <- c("<c>","<p>","<S>","<l>","<q>","<d>","<Q>")
for (i in tags){
papers$text <- gsub(i," ",papers$text)
}
# build the corpus
myCorpus <- corpus(papers$text)
# add in the attributes about the papers (number, author, train/test flag)
docvars(myCorpus, "Number") <- papers$number
docvars(myCorpus, "Author") <- papers$author
# summarize the documents
summary(myCorpus, 5)
stopWords <- c("will","one","two","may","less", "well","might","without","small","single",
"several","however","must","number","part","upon","consider","particular","place","true","shall","often",
"former","latter","want","can","everything","every","different","either","yet","made","now","publius",
"therefore","first","second","third","though","another","found","within","even","far","just","also",
"said","ever","three","four","still","little","federal","members","national","union","united","general",
"government","governments","power","powers","state","states","people","constitution","constitutions")
myDfm <- dfm(myCorpus, ignoredFeatures = c(stopwords("english"),stopWords), stem = T, ngrams = c(1,3))
hamilton.author <- ifelse(papers$author[train]=="HAMILTON",1,0)
library(glmnet)
require(doMC)
registerDoMC(cores=3)
ridge <- cv.glmnet(myDfm[train,], hamilton.author,
family="binomial", alpha=0, nfolds=5, parallel=TRUE,
type.measure="deviance")
plot(ridge)
dev.off()
plot(ridge)
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(ridge, twdfm[train,], type="response") > mean(hamilton.author)
preds <- predict(ridge, myDfm[train,], type="response") > mean(hamilton.author)
table(preds, hamilton.author)
myDfm <- trim(myDfm, minCount=5, minDoc=5)
hamilton.author <- ifelse(papers$author[train]=="HAMILTON",1,0)
dev.off()
library(glmnet)
require(doMC)
registerDoMC(cores=3)
ridge <- cv.glmnet(myDfm[train,], hamilton.author,
family="binomial", alpha=0, nfolds=5, parallel=TRUE,
type.measure="deviance")
plot(ridge)
accuracy <- function(ypred, y){
tab <- table(ypred, y)
return(sum(diag(tab))/sum(tab))
}
# function to compute precision
precision <- function(ypred, y){
tab <- table(ypred, y)
return((tab[2,2])/(tab[2,1]+tab[2,2]))
}
# function to compute recall
recall <- function(ypred, y){
tab <- table(ypred, y)
return(tab[2,2]/(tab[1,2]+tab[2,2]))
}
# computing predicted values
preds <- predict(ridge, myDfm[train,], type="response") > mean(hamilton.author)
View(preds)
table(preds, hamilton.author)
accuracy(preds, hamilton.author)
precision(preds, hamilton.author)
recall(preds, hamilton.author)
predict(ridge, myDfm[test,], type="response")
mean(hamilton.author)
disputed.papers <- predict(ridge, myDfm[test,], type="response") > mean(hamilton.author)
View(disputed.papers)
print(disputed.papers)
disputed.papers <- ifelse(disputed.papers == 1, "HAMILTON", "MADISON")
View(disputed.papers)
best.lambda <- which(ridge$lambda==ridge$lambda.min)
beta <- ridge$glmnet.fit$beta[,best.lambda]
head(beta)
## identifying predictive features
df <- data.frame(coef = as.numeric(beta),
word = names(beta), stringsAsFactors=F)
df <- df[order(df$coef),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
df <- df[order(df$coef, decreasing=TRUE),]
head(df[,c("coef", "word")], n=30)
paste(df$word[1:30], collapse=", ")
